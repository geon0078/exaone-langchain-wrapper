# chat.py

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, TextIteratorStreamer
from config import MODEL_NAME, MAX_NEW_TOKENS, SYSTEM_PROMPT, SAVE_HISTORY, HISTORY_FILE
from save_chat import save_chat_history
from chat_memory import ChatMemory
import threading
import sys

def main():
    device = "cuda" if torch.cuda.is_available() else "cpu"

    print(f"ğŸ”§ ë””ë°”ì´ìŠ¤ ì„¤ì •: {device}")

    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
    model = AutoModelForCausalLM.from_pretrained(
        MODEL_NAME,
        torch_dtype=torch.bfloat16 if device == "cuda" else torch.float32,
        trust_remote_code=True,
        device_map=device
    )

    memory = ChatMemory(system_prompt=SYSTEM_PROMPT, tokenizer=tokenizer, max_total_tokens=2048)

    chat_log = []  # íŒŒì¼ë¡œ ì €ì¥í•  ë•Œ ì“¸ ê¸°ë¡
    print("ğŸ¤– EXAONE ë©”ëª¨ë¦¬ ì±—ë´‡ì— ì˜¤ì‹  ê±¸ í™˜ì˜í•©ë‹ˆë‹¤! (ì¢…ë£Œí•˜ë ¤ë©´ 'exit' ì…ë ¥)")

    try:
        while True:
            try:
                user_input = input("\nğŸ‘¤ ë‹¹ì‹ : ").strip()
            except (KeyboardInterrupt, EOFError):
                print("\n\nğŸ‘‹ Ctrl+C ê°ì§€. ì±—ë´‡ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                if SAVE_HISTORY and chat_log:
                    save_chat_history(chat_log, HISTORY_FILE)
                    print(f"ğŸ’¾ ëŒ€í™” ê¸°ë¡ì´ {HISTORY_FILE} íŒŒì¼ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
                sys.exit(0)

            if not user_input:
                print("âš ï¸ ì…ë ¥ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì…ë ¥í•´ ì£¼ì„¸ìš”.")
                continue

            if user_input.lower() in ["exit", "quit", "ì¢…ë£Œ"]:
                print("ğŸ‘‹ ì±—ë´‡ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                if SAVE_HISTORY and chat_log:
                    save_chat_history(chat_log, HISTORY_FILE)
                    print(f"ğŸ’¾ ëŒ€í™” ê¸°ë¡ì´ {HISTORY_FILE} íŒŒì¼ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
                break

            # memoryì— ìˆëŠ” ëŒ€í™” + ì´ë²ˆ ì§ˆë¬¸ì„ í•©ì³ì„œ ëª¨ë¸ input ìƒì„±
            messages = memory.get_memory_with_prompt() + [{"role": "user", "content": user_input}]

            try:
                input_ids = tokenizer.apply_chat_template(
                    messages,
                    tokenize=True,
                    add_generation_prompt=True,
                    return_tensors="pt"
                ).to(device)

                # ìŠ¤íŠ¸ë¦¬ë¨¸ ìƒì„±
                streamer = TextIteratorStreamer(tokenizer, skip_special_tokens=True)

                # ëª¨ë¸ ìƒì„± í˜¸ì¶œì„ ë³„ë„ì˜ ìŠ¤ë ˆë“œì—ì„œ ì‹¤í–‰
                generation_thread = threading.Thread(
                    target=model.generate,
                    kwargs={
                        "input_ids": input_ids,
                        "eos_token_id": tokenizer.eos_token_id,
                        "max_new_tokens": MAX_NEW_TOKENS,
                        "do_sample": False,
                        "streamer": streamer,
                    },
                )
                generation_thread.start()

                # ìŠ¤íŠ¸ë¦¬ë°ëœ í† í° ì¶œë ¥
                print("\nğŸ¤– EXAONE: ", end="", flush=True)
                response = ""
                for token in streamer:
                    print(token, end="", flush=True)
                    response += token
                print()

                # ëŒ€í™” ê¸°ë¡ ì—…ë°ì´íŠ¸
                memory.add_turn(user_input, response)
                chat_log.append({"user": user_input, "exaone": response})

            except Exception as e:
                print(f"\nğŸš¨ ëª¨ë¸ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                print("ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”.")
                continue

    except Exception as e:
        print(f"\nğŸš¨ ì˜ˆê¸°ì¹˜ ëª»í•œ ì˜¤ë¥˜ ë°œìƒ: {e}")
        if SAVE_HISTORY and chat_log:
            save_chat_history(chat_log, HISTORY_FILE)
            print(f"ğŸ’¾ ëŒ€í™” ê¸°ë¡ì´ {HISTORY_FILE} íŒŒì¼ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        sys.exit(1)

if __name__ == "__main__":
    main()
